# Text to Image Generation with Semantic-Spatial Aware GAN

## 作者：Kai Hu，Wentong Liao，Michael Ying Yang，Bodo Rosenhahn 时间：2022

## 会议：CVPR

## 为什么提出SSA-GAN:

- 最近的T2I工作在生成的图像上有两个主要局限性：

  1）condition batch normalization 方法同样适用于整个图像特征映射，忽略了局部语义。

  2）文本编码器在训练过程中是固定的，应该与图像生成器一起训练，以学习更好的文本表示，以便生成图像。

- 具体来说：

  - 当前T2I模型融合文本和图像信息的方法主要有三种：特征拼接、跨模态注意和条件批量归一化：

    1）特征拼接：比如StackGAN、StackGAN++等只是通过朴素的拼接实现文本图像融合的，**这样既没有充分利用文本信息，也没有有效的文本图像融合。**

    2）跨模态注意：如AttnGAN，该方法旨在为图像的每个子区域计算单词上下文向量。但是这样的方法**随着图像尺寸的增大，计算量迅速增加。此外，自然语言描述是高级语义的，而图像的一个子区域是相对低级的。因此，它无法很好地挖掘高层语义来控制图像生成过程，尤其是对于具有多个对象的复杂图像。**

    3）条件批量归一化（CBN）：如SD-GAN，该论文提出了单词级和句子级CBN，将文本信息注入到图像特征图中。**但是它们的CBN在图像生成过程中只应用了几次，使得文本特征和图像特征没有充分融合。**在DF-GAN中，一系列叠加仿射变换用于按通道缩放和移动图像特征。**然而，它们的仿射变换在空间上对特征地图的作用是相同的。理想情况下，文本信息只应添加到与文本相关的子区域。**

  - 另外作者认为在训练过程中固定了预先训练好的文本编码器是超优的，而如果文本编码器可以与图像生成器联合训练，则可以更好地利用文本信息生成图像。

- **因此总的来说，现有的文本图像融合方法无法将文本信息深入有效地融合到视觉特征图中，以控制给定文本的图像生成过程。**

## SSA-GAN的贡献：

- 为了解决上述的问题，作者提出了一种新的T2I框架，称为语义空间感知生成对抗网络（SSA-GAN），其主要贡献为：
  - 提出一种新的SSA-GAN框架，该框架可以以端到端的方式进行训练，以便文本编码器能够学习更好的文本表示以生成更好的图像特征。
  - 引入一种新的SSACN块，通过预测空间遮罩图来指导学习的文本自适应仿射变换，有效而深入地融合文本和图像特征。SSACN块以弱监督的方式进行训练，因此不需要额外的注释。

## SSA-GAN模型结构：

- ![image-20220630163100900](D:\workplace\note\6月份\Text to Image Generation with Semantic-Spatial Aware GAN_img\image-20220630163100900.png)
- 可以看到，SSA-GAN有一个生成器鉴别器对（遵循DFGAN提出的一阶段），该网络的核心元素是语义空间感知卷积网络SSACN，其由语义空间条件批量归一化SSCBN的CBN模块、残差块和掩码预测器组成。
- SSA-GAN模型遵循DF-GAN中提出的单阶段结构，SSA-GAN具有学习文本表示的文本编码器、具有7个SSACN块的生成器，用于深化文本图像融合过程和提高分辨率，以及用于判断生成的图像是否与给定文本一致的鉴别器。

### 1. Text Encoder：

- 采用的是AttnGAN提供的预训练文本编码器，该文本编码器是一种双向LSTM，通过最小化深度注意多模态相似模型损失（DAMSM），使用真实图像-文本对进行训练。

### 2. SSACN：

- ![image-20220721101248034](D:\workplace\note\6月份\Text to Image Generation with Semantic-Spatial Aware GAN_img\image-20220721101248034.png)
- 如上图所示，它采用编码文本特征向量e和来自最后一个SSACN块的图像特征映射$f_i$作为输入，并输出图像特征$f_i$，且进一步与文本特征融合。其中第一个SSACN块（无上采样）的输入图像特征图的形状为4x4x512，通过使用FC层将噪声矢量z投影到视觉域，然后对其进行整形来实验。因此，经SSACN块6次上采样后，图像特征图具有256x256分辨率。
- 每个SSACN块由上采样块、掩码预测器、语义空间条件批量归一化（SSCBN）和残差块组成。使用上采样块通过双线性插值操作将图像特征图的宽度和高度加倍。残差块用于保持图像特征的主要内容，以防止文本无关部分被更改，并且图像信息被文本信息淹没。

#### 2.1 Weakly-supervised Mask Predictor：

- 掩码预测器的结构在上图中灰色虚线框突出显示的部分。它将上采样的图像特征映射作为输入，并预测掩膜映射$m_i \in R^{h_i \times w_i}$，它的任何一个$m_i$的值在[0,1]之间。该值决定了以下仿射变换应在位置(h,w)上操作的程度。该映射基于当前生成的图像特征映射进行了预测。
- 因此，它直观地指示当前图像特征图的哪些部分仍然需要使用文本信息进行增强，以便改进后的图像特征图与给定文本更具语义一致性。掩码预测器与整个网络联合训练，没有特定的损失函数来指导其学习过程，也没有额外的掩码注释。唯一的监督来自鉴别器给出的对抗性损失。

#### 2.2 Semantic-Spatial Condition Batch Normalization

- ![image-20220726172346531](D:\workplace\note\6月份\Text to Image Generation with Semantic-Spatial Aware GAN_img\image-20220726172346531.png)
- 从公式中可以看出，$m_{i,(h,w)}$不仅决定在何处添加文本信息，而且还作为权重来决定在图像特征图上增强多少文本信息。
- 根据文本信息学习调制参数$\gamma$和$\beta$，预测的掩膜映射在空间上控制仿射变换。因此，为了融合文本和图像特征，实现了语义空间CBN。

## 3. Discriminator：

- 由于其有效性和简单性，该文采用了DF-GAN中提出的单向鉴别器。

  ![image-20220726235521562](D:\workplace\note\6月份\Text to Image Generation with Semantic-Spatial Aware GAN_img\image-20220726235521562.png)

- 它将从生成的图像中提取的特征和编码的文本向量连接起来，通过两个卷积层计算对抗损失。它引导生成器合成更真实的图像，具有更好的文本图像语义一致性。

- 另外为了进一步提高生成图像的质量和文本图像的一致性，并帮助与生成器一起训练文本编码器，作者将广泛应用的DAMSM添加到我们的框架中。（注意，即使没有DAMSM，我们的方法也已经报告了最先进的性能状态。）

## 4. 实验：

- ![image-20220803100252795](D:\workplace\note\6月份\Text to Image Generation with Semantic-Spatial Aware GAN_img\image-20220803100252795.png)

- ![image-20220803100325578](D:\workplace\note\6月份\Text to Image Generation with Semantic-Spatial Aware GAN_img\image-20220803100325578.png)

  