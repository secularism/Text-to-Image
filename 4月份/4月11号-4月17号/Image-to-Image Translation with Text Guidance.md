# Image-to-Image Translation with Text Guidance

## 作者：Bowen Li、Xiaojuan Qi、Philip H. S. Torr、Thomas Lukasiewicz  时间：2020

## 会议/期刊：CVPR

## 为什么提出该文（RefinedGAN）：

* 该文和以下的工作有联系：

  **Image-to-image translation**：该领域的工作和作品只专注于从像素标记的语义图像生成逼真的图像，而不能确定合成图像的视觉属性。

  **Text-to-image generation**：在T2I领域中主要集中在从文本描述或场景图中生成逼真的图像，往往不能再更复杂的数据集上产生高质量的结果，如COCO数据集。

  **Text-guided image manipulation**：多数方法主要是为了编辑给定的图像，而不是产生新的结果

  **Multi-stage architectures**：与之前模型中的多级体系结构不同的是，作者提出的模型充分利用了再较高阶段产生的特征，将这些特征输入到较低阶段的鉴别器中，以提高鉴别器的鉴别能力，这进一步有利于生成器提高其整流能力

## RefinedGAN的目标：

* 该文的目标是将可控因素，即自然语言描述，嵌入到生成对抗网络的图像到图像的翻译中，通过分割蒙版生成逼真的图像，控制合成图像的视觉属性（如颜色、背景、纹理）与给定文本的语义匹配。

* 与目前的I2I转换模型不同，它需要细粒度的像素标记语义图像来决定**生成什么**。如下图所示，给定一个简单的圆形分割蒙版，该模型可以生成一个草地区域的停车标志，也可以生成一个奶酪和意大利辣香肠披萨。

  ![image-20220414112618028](./Image-to-Image%20Translation%20with%20Text%20Guidance_img/image-20220414112618028.png)

* 要实现这一点，关键是要将文本描述和图像中所包含的不同视觉属性完全理清，然后在语义词和对应的视觉属性之间建立精确的关联，实现有效的控制，并且如何在更困难的数据集上有效地生成涉及不同模态表示的逼真图像也是一个需要解决的关键问题，**其中数据集中的每一幅图像都有多个彼此之间具有复杂关系的对象**。

* 因此提出了RefinedGAN，它有四个关键组成部分：

  a）词性标注（Part-Of-Speech Tagging）的实现用以过滤给定描述中的非语义词；

  b）采用ACM模块（ManiGAN中的）融合不同模态文本和图像特征；

  c）新的精细多阶段体系结构，增强鉴别器的鉴别能力和生成器的校正能力；

  d）新的结构损失，增强鉴别器能力，更好地区分真实和合成图像。

## 模型结构：

* ![image-20220414113524108](./Image-to-Image%20Translation%20with%20Text%20Guidance_img/image-20220414113524108.png)

* 该模型采用ControlGAN作为基本框架。

  **首先**使用POS来过滤出给定文本描述中的非语义词；

  **然后**将输出输入一个预先训练的RNN来生成文本表示，在每一个阶段采用一个ACM模块中，将前一阶段生成的文本特征与蒙版融合，这可以在单词与蒙版对应的语义部分之间建立准确的相关性，并因此将文本信息嵌入到生成过程中，从而实现有效的可控能力；

  **最后**通过残差块和上采样块对融合后的特征进行细化，生成隐藏特征。并将隐藏特征送入发生器输出合成图像，同时作为下一阶段的输入，生成更高分辨率的图像。

### Part-of-Speech Tagging：

* 在文本描述中，可能会包含一些不太重要的词，这些词不能帮助生成图像，甚至会造成负面影响，比如上图描述中的“a，to，its”没有任何语义意义。因此为了过滤这些掉这些话，作者实现了POS标记标签描述每个单词。

* POS以文本描述作为输入，给每个单词贴上相应的标签：

  ![image-20220414125958768](./Image-to-Image%20Translation%20with%20Text%20Guidance_img/image-20220414125958768.png)

* 这里的\*代表任何元素出现零次或多次，NN\*表示所有名词的不同形式，IN\*表示介词或从属连词，VB\*表示所有形式的动词，JJ\*表示所有的形容词，在模型中，只保留这些特定的词。因为名词、介词和动词已经抓住了句子的主要意思，而形容词包含了对图像视觉属性的主要描述。

  ##### **为什么过滤掉不太重要的单词比保留所有单词效果更好? **

* 在给定的文本描述中，并不是所有的单词都是同等重要的，有些单词可能对生成过程没有或甚至是负面的影响，此外在保留这些单词的情况下，基于RNN的文本呢编码器生成的单词和句子特征就得包含这些不太有用的信息，然后再非语义单词和视觉属性之间建立不恰当的关联。因此，控件的性能可能会受到影响，特别是当用户只想修改合成图像中某些视觉属性而保留其他内容时。

### Affine Combination Module（ACM）模块

* [参考ManiGAN](https://blog.csdn.net/qq_44381060/article/details/124067205?spm=1001.2014.3001.5501)

### Refined Multi-Stage Architecture：

### 1. 为什么提出Refined Multi-Stage Architecture：

* 作者指出对于生成模型来说，在更困难的数据集上生成涉及不同模态表示的逼真图像是一个巨大的挑战，甚至使用多阶段的架构（multi-stage），也是困难的，multi-stage在第一阶段生成粗图像，然后以更细的细节逐步增加其分辨率。

* 以往的multi-stage的initial images会生成不恰当的情况，其原因主要是：

  1）这些模型在较低阶段，特别是第一个阶段，生成的基本结构不完整，这意味着在第一个阶段生成的合成图像的某些部分是不现实的

  2）生成器拳法弥补缺失的细节或纠正不合适的视觉属性的能力。

  因此，由于基本图像的缺陷和生成器的效率较低，该模型无法生成具有真实细节的高质量图像。

### 2. Refined Multi-Stage Architecture介绍：

* 为了解决上述问题，作者提出了一种新的multi-stage，如上图2中，该结构可以充分挖掘单个图像中斑块的内部分布，从而增强鉴别器在较低阶段的鉴别能力和生成器的校正能力。

* 该multi-stage每一级有一个发生器和一个鉴别器，逐步生成不同尺度的图像，合成图像的分辨率是前一幅图像的**4倍**，为了生成一个完整的结构与细节，作者向低层次的discriminator送入了高层次的real和fake图片块，这些图片块包含看不见但却精细的细节，可以作为额外信息来帮助训练低层次discriminator的辨别能力，从而反过来训练generator生成更精细的图片，特别是第一阶段的generator。因此，第$i$阶段discriminator的额外无条件对抗损失$L_{Z_{Di}}$定义为：

  > ![image-20220415110733313](./Image-to-Image%20Translation%20with%20Text%20Guidance_img/image-20220415110733313.png)

  上式中$K$是总阶段数，$P^`_K$和$P_k$分别为合成图像$I^`_k$和真实图像$I_k$在较高阶段$k$时的随机patch，$P^`_K$和$P_k$的大小匹配discriminator $D_i$的输入要求。第$i$阶段generator的额外无条件对抗损失$L_{Z_{Gi}}$定义为

  > ![image-20220415110905375](./Image-to-Image%20Translation%20with%20Text%20Guidance_img/image-20220415110905375.png)

  这里$i>1$，$P^`_K$是第$i$级合成图像$I_i^`$的一个非分离的随机patch。$P^`_K$的裁剪大小与discriminator$D_k$的输入要求相匹配。

  ##### **为什么这样做多阶段结构会更好**

* 这个体系结构的目标是通过将补丁从较高的阶段提供给较低的阶段来优化鉴别器和生成器。在较低阶段，这些补丁包含一个不可见的内部分布，具有细粒度的细节。**这样做，在较低阶段的鉴别器可以根据全局分布和区域特征更好地识别假图像，这反过来鼓励在相同阶段的生成器生成更精细的真实图像。**此外，这些增强的鉴别器可以在接下来的阶段向生成器**提供区域反馈**，完善生成器以生成真实的区域，**并具有完成缺失内容和纠正不适当的视觉属性的能力。**

### Structure Loss：

* 为了进一步提高鉴别器的鉴别能力，作者提出了一种新的结构损失，它也可以用来稳定训练。它可以使用所提供的分割蒙版来分离合成图像和真实图像上的对象和背景。然后根据不同的对象和背景创建出新的成分（这里不同的背景和对象，分为真假背景和真假对象），也就是说创建假的对象+真实背景或是真实对象+假的背景，并将他们输入feed到discriminator中以提升改善他们的鉴别能力，比如识别生成图像如果存在一些不切实际的背景。反过来，也可以鼓励生成器在任何地方生成更精细的细节，而不是只专注于生成真实对象或背景。因此，第$i$阶段的损失$L_{S_i}$为：

  > ![image-20220415145416601](./Image-to-Image%20Translation%20with%20Text%20Guidance_img/image-20220415145416601.png)

  其中，$X_i^1$表示由带有真实背景的假物体组成的新图像，$X_i^2$表示阶段$i$带有假背景的真实物体。

## 实验：

* ![image-20220415145758325](./Image-to-Image%20Translation%20with%20Text%20Guidance_img/image-20220415145758325.png)

* 上图代表与在COCO数据上与最先进的方法作定量比较，

  其中“w/o POS”表示不加词性标注；

  “w/Concat”表示使用连接方法来组合文本和图像（也就是不使用ACM）；

  “w/o Refined”表示在generator和discriminator上不适用改进后的multi-stage架构；

  “w/o SL”表示无结构损失，“w/o POS\*”表示不使用POS对模型进行训练，但在测试时实现POS。

  为了公平比较，作者通过实现ACM略微修改了AttnGAN和ControlGAN，加入了segmentation mask，并将修改后的命名为S-AttnGAN和S-ControlGAN。

* 可以看到RefinedGAN的效果更好，并且具有较高的多样性。其中R-prcn值越高，说明我们的模型生成的合成图像与给定的文本描述高度匹配。

* ![image-20220415150552609](./Image-to-Image%20Translation%20with%20Text%20Guidance_img/image-20220415150552609.png)

* 上图表示三种方法在COCO数据集上的定性比较

  1）a和b表示在相似的分割蒙版上不同类别的对象的生成。

  2）c和d说明了对象内部视觉属性的可控能力;

  3）e和f显示了在合成图像上添加新的视觉属性的能力，同时保留了其他未修改的文本内容;

  4）g和h表明，该模型还能控制生成结果的全局风格。

* ![image-20220415151048245](./Image-to-Image%20Translation%20with%20Text%20Guidance_img/image-20220415151048245.png)
* 图4则表明如果没有提供segmentation mask，模型只生成background，但是结果仍然在语义上匹配给定的描述。这说明有效的解开了前景和背景。

## Ablation Studies:

### Necessity of part-of-speech tagging.（词性标注的必要性）：

* ![image-20220415151256522](./Image-to-Image%20Translation%20with%20Text%20Guidance_img/image-20220415151256522.png)
* 如图6所示，当从描述中去除非语义词时，合成图像中的一些区域就变得不现实了，比如公交车上出现了一个明显的斑痕，另外无用的文字甚至会降低合成结果的质量，比如会降低蓝天的亮度，影响色彩斑斓的风筝的质感。

### Effectiveness of affine combination module（ACM模块的有效性）：

* ![image-20220415151519942](./Image-to-Image%20Translation%20with%20Text%20Guidance_img/image-20220415151519942.png)
* 如图5c和f所示，可以明显的看出，采用拼接而不是ACM来结合不同的模态特征，模型不能产生更精细的真实图像，甚至不能保持给定描述的语义一致性。这主要是因为简单的拼接并不能将语义词与图像对应区域建立准确的连接，也不能将可控的文本描述有效地编码到生成过程中。

### Effectiveness of refined multi-stage architecture（精炼multi-stage的有效性）：

* ![image-20220415151801039](./Image-to-Image%20Translation%20with%20Text%20Guidance_img/image-20220415151801039.png)
* 如图7所示，可以很清楚的没有renfined multi-stage的模型在第一阶段（c图）比有的（f图）少了很多精细的部分，此外，后续阶段的生成器无法完成缺失的内容或纠正不合适的属性。

### Effectiveness of structure loss（结构损失的有效性）：

* 如图5e所示，没有结构损失的模型所产生的合成图像模型包含一些不切实际的地区，比如，有一些不切实际的补丁在公共汽车上和飞机的外观。这可能表明，如果物体或背景中存在小的不合理区域，结构损失可以提高鉴别器识别假图像的鉴别能力，并反过来改进generator，以产生更高质量的图像和更精细的细节。