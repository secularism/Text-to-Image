# ManiGAN: Text-Guided Image Manipulation

## 作者：Bowen Li、Xiaojuan Qi、Thomas Lukasiewicz、Philip H. S. Torr  时间：2020

## 期刊：CVPR

## 提出该网络的目标任务：

* 论文的目标是在语义上编辑图像的部分，以匹配描述所需属性（比如，纹理、颜色和背景）的给定文本，同时保留与文本无关的其他内容。目前来说，现最先进的文本引导图像处理方法只能产生低质量的图像（下图第一行），效果不理想（下图第二行），甚至不能有效地处理复杂场景，要在文本描述的指导下对图像进行有效的处理，关键是同时利用文本和图像的跨模态信息，生成与给定文本匹配的新属性，同时保留与文本无关的原始图像内容。
* ![image-20220408105857810](D:\workplace\note\4月份\4月4号-4月10号\ManiGAN Text-Guided Image Manipulation_img\image-20220408105857810.png)

## 现阶段其他方法的弊端：

* 现有的方法通常选择沿通道方向直接拼接图像和全局句子特征。尽管很简单，但是作者认为这样的方法可能会遇到一些潜在问题。首先，模型无法将细粒度词与相应需要修改的视觉属性精确关联，导致修改不准确、粗化（比如上图图一中两个网络都不能生成详细的视觉属性——黑眼圈和黑喙）。其次，模型不能有效地识别出与文本无关的内容，从而不能对其进行重构，导致图像中与文本无关的部分被修改（比如上图图一中两个模型除了修改所需的属性外，还改变了第一行小鸟的纹理和第二行场景的结构）。因此，针对上述问题，作者们提出了一种新颖的文本引导图像处理生成对抗网络ManiGAN。

## ManiGAN：

* 该网络包含两个关键组件：文本图像仿射组合模块（ACM）和细节校正模块（DCM）。
  * ACM选择与给定文本相关的图像区域，然后将这些区域与相应的语义词相关联以进行有效操作。同时，对原始图像特征进行编码，帮助重建与文本无关的内容。
  * DCM对合成图像中不匹配的属性进行校正，并补全缺失的内容。
* 最后，作者提出了一个新的metric来评估图像处理的结果。无论是在给定文本对应的新的视觉属性的生成，还是原始图像与文本无关的内容的重构。该metric可以很好地反映图像处理的性能。

### 模型结构：

* ![image-20220409155113392](D:\workplace\note\4月份\4月4号-4月10号\ManiGAN Text-Guided Image Manipulation_img\image-20220409155113392.png)
* 该模型通过给定输入图像$I$和用户提高的文本描述$S^`$，模型的目标是生成一个经过操作的图像$I^`$，该图像$I^`$与$S^`$语义对齐，同时保留$I$存在的与文本无关的内容。（上图是简略后的模型图）
* ManiGAN使用多级ControlGAN架构作为基本框架，在输入部分与ControlGAN不同的地方是增加了图像编码器，它是一个预先训练的Inception-v3网络，用于提取区域图像表示$v$。在每一个阶段，text feature通过卷积层进行细化，生成隐藏特征$h$，然后，通过ACM模块进一步将$h$与原始图像特征$v$相结合，以便有效地选择与给定文本对应的图像区域，然后将这些区域与文本信息关联起来进行精确的操作。同时对原始图像表示进行编码，实现稳定重建。整个框架以更高的分辨率和更高的质量逐渐生成与给定文本描述匹配的新的视觉属性，并以更细的尺度重建输入图像中存在的与文本无关的内容。最后，利用所提出的细节校正模块(DCM)来校正不合适的属性，并补全缺失的细节。

### Text-Image Affine Combination Module（ACM）：

* 作者指出现有的结合textimage跨模态表示的连接方案不能有效地定位需要修改的区域，因此无法实现细粒度的图像操作。因此提出一个简单的文本-图像仿射组合模块来融合文本跨模态表示。

* ![image-20220409161626809](D:\workplace\note\4月份\4月4号-4月10号\ManiGAN Text-Guided Image Manipulation_img\image-20220409161626809.png)

* 该模块有两个输入（1）来自文本或者两个阶段之间的隐藏表示 hidden feature $h$ （2）区域图像特征$v$，然后对$v$做上采样，然后进一步使用两个卷积层进行处理，得到与$h$大小相同的$W(v)$和$b(v)$，最后，通过融合两种模态表示得到$h^`$。

  > $h^`=h \odot W(v) +b(v)$

  其中$W(v)$和$b(v)$是根据区域图像特征$v$学习到的权重和偏差，$\odot$为Hadamard element-wise product。用$W$和$b$来表示将区域特征$v$转换为缩放值和偏差值的函数。

### Detail Correction Module（DAM）：

* ![image-20220409165301647](D:\workplace\note\4月份\4月4号-4月10号\ManiGAN Text-Guided Image Manipulation_img\image-20220409165301647.png)
* 该模块有三个输入（1）来自最后一个ACM模块的隐藏feature $h_{last}$，（2）由预训练RNN编码的单词特征word feature，其中每个单词与一个特征向量相关联。（3）从输入图像$I$中提取的视觉特征$v^`$。
* 首先，通过ControlGAN中引入的空间注意和通道注意生成空间和通道注意特征$s$和$c$，再进一步将细粒度的词级表示整合到隐藏特征$h_{last}$中，生成中间特征$a$。**特征$a$可以进一步帮助模型细化与给定文本相关的视觉属性，有助于更准确和有效地修改与给定描述对应的内容。**其次，利用预训练的VGG网络的relu2_2层得到的特征通过上采样使其与$a$的大小相同，表示为$\overline{v}^`$。之后利用ACM模块融合$a$和$\overline{v}^`$，得到$\overline{a}$。最后，采用用两个残差块(细节在补充材料中)细化图像得到$I^`$。

## 实验：

* ![image-20220409171154337](D:\workplace\note\4月份\4月4号-4月10号\ManiGAN Text-Guided Image Manipulation_img\image-20220409171154337.png)
* ![image-20220409171212658](D:\workplace\note\4月份\4月4号-4月10号\ManiGAN Text-Guided Image Manipulation_img\image-20220409171212658.png)
* ![image-20220409171219969](D:\workplace\note\4月份\4月4号-4月10号\ManiGAN Text-Guided Image Manipulation_img\image-20220409171219969.png)

## Manipulative precision metric：

* 作者认为使用自然语言描述的图像处理应该从两个方面来评估：

  * 从给定文本生成新的视觉属性
  * 重建存在于输入图像中的原始内容（也就是没有被修改部分）

* 但是现有的度量标准只关注这个问题的一个方面，比如$L_1$ Eu clidean距离、峰值信噪比、SSIM仅衡量两幅图像之间的相似度，而余弦相似度和检索精度仅衡量文本与相应生成图像之间的相似度。在此基础上，作者提出了一种新的度量方法——操纵精度(manipulative precision, MP)，用于同时测量生成和重构的质量。定义为：

  > $MP=(1-diff) \times sim$

  其中$diff$是$L_1$像素差异输入图像和相应的修改图像，$sim$是textimage相似，它是基于文本图像匹配分数，利用预先训练的文本图像编码器，提取给定文本描述和相应修改图像的全局特征向量，然后利用这两个全局向量的余弦相似度计算相似度值。具体来说，本设计基于的直觉是，如果经过处理的图像是由身份映射网络生成的，那么文本-图像的相似度应该很低，因为合成的图像不能很好地与给定的文本描述保持语义的一致性。

